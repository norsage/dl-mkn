{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Организационная информация\n",
    "\n",
    "\n",
    "Основная коммуникация: Telegram `DL - СПбГУ - 2023`. Если вас там нет, напишите `@snikolenko`, чтобы он вас туда добавил.\n",
    "\n",
    "Материалы и домашние работы: `emkn`, GitHub: https://github.com/norsage/dl-mkn.git\n",
    "\n",
    "Аттестация: по результатам результатов решений нескольких прикладных задач, где потребуется побить бейзлайн\n",
    "\n",
    "Упражнения после занятия: опциональны, но очень полезны для развития интуиции\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. План на сегодня: знакомство с PyTorch\n",
    "\n",
    "1. Тензоры и операции над ними\n",
    "2. Граф вычислений и автоматическое дифференцирование\n",
    "3. Реализация новых операций с помощью `torch.autograd.Function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Подготовка окружения\n",
    "\n",
    "1) Локально с помощью conda\n",
    "   \n",
    "   (можно установить по инструкции https://docs.conda.io/projects/miniconda/en/latest/index.html#quick-command-line-install)\n",
    "   ```bash\n",
    "   # создаём окружение dl-course\n",
    "   conda create -n dl-course -y python=3.10\n",
    "   # активируем окружение\n",
    "   conda activate dl-course\n",
    "   # устанавливаем Jupyter\n",
    "   conda install jupyter notebook -c conda-forge\n",
    "\n",
    "   # Устанавливаем pytorch (https://pytorch.org/get-started/locally/)\n",
    "   # Пример для Linux / Windows с поддержкой GPU:\n",
    "   conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "   # Для MacOS с ускорением MPS:\n",
    "   # conda install pytorch::pytorch torchvision torchaudio -c pytorch\n",
    "   ```\n",
    "\n",
    "   Запустить ноутбук можно как в браузере:\n",
    "   \n",
    "   `jupyter notebook 01_pytorch_intro.ipynb`\n",
    "\n",
    "   так и в VSCode, просто открыв файл и выбрав кернел для исполнения в правом верхнем углу (потребуется поставить расширения `Python` и `Jupyter`)\n",
    "2) Используем Сolab https://colab.research.google.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PyTorch basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Тензоры, способы создания и атрибуты\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать тензор можно многими способами:\n",
    "1. Напрямую из объектов\n",
    "2. Из массивов `numpy`\n",
    "3. Из других тензоров\n",
    "4. С константными и случайными значениями\n",
    "5. Используя специальные функции для особых случаев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные атрибуты: ранг (`dim`), размерности (`shape`), тип значений (`type`), место размещения (`device`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Rank:  2 \n",
      "Shape:  torch.Size([2, 3]) \n",
      "Device:  cpu \n",
      "type:  torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# из списка\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# посмотрим сам тензор и его атрибуты\n",
    "print(x)\n",
    "print(\n",
    "    \"\\nRank: \", x.dim(),\n",
    "    \"\\nShape: \", x.shape,\n",
    "    \"\\nDevice: \", x.device,\n",
    "    \"\\ntype: \", x.type(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# создание из numpy\n",
    "arr = np.random.normal(size=(3, 2))\n",
    "x = torch.from_numpy(arr)  # shape = (3, 2)\n",
    "\n",
    "# из других тензоров, объединяя несколько в один\n",
    "y = x + 1\n",
    "z = torch.cat([x, y], dim=0)  # shape = (6, 2)\n",
    "z = torch.cat([x, y], dim=1)  # shape = (3, 4)\n",
    "\n",
    "# константы и специальные случаи\n",
    "x = torch.ones(size=(4,), dtype=bool)\n",
    "x = torch.eye(4, dtype=int)\n",
    "x = torch.linspace(start=0, end=5, steps=11)\n",
    "\n",
    "# случайные величины\n",
    "x = torch.rand(size=(4,))  # U[0, 1]\n",
    "x = torch.randn(size=(2, 2))  # N(0, 1)\n",
    "\n",
    "\n",
    "# если нужно зафиксировать seed\n",
    "torch.manual_seed(4)\n",
    "x = torch.bernoulli(\n",
    "    torch.full((3, 2), 0.2)\n",
    ")\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Простые операции, функции, линейная алгебра\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Различных операций сотни, можно выделить несколько групп по назначению:\n",
    "- создание (`linspace`, `arange`, `full`)\n",
    "- индексация, срезы, объединения (`argwhere`, `cat`, `stack`, `tile`, `gather`)\n",
    "- генерация случайных величин (`normal`, `bernoulli`, `randperm`)\n",
    "- сериализация / десериализация (`save`, `load`)\n",
    "- математические операции (`log`, `deg2rad`, `clamp`, `sigmoid`)\n",
    "- агрегирование (`sum`, `mean`, `argmax`, `quantile`)\n",
    "- линейная алгебра (`torch.linalg.*`)\n",
    "- FFT (`torch.fft.*`)\n",
    "- обработка сигналов (`torch.signal.*`)\n",
    "- нейронные сети (`torch.nn.*`)\n",
    "- ...\n",
    "\n",
    "Многие операции также реализованы как методы класса `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12).view((-1, 3, 2))  # shape: (2, 3, 2)\n",
    "# срезы\n",
    "x[0, 1:3, :2]  # shape: (2, 2)\n",
    "# берём всё из ведущих размерностей, из последней - только второй элемент\n",
    "x[:, :, 1]  # shape: (2, 3)\n",
    "# лень преречислять ведущие размерности - используем эллипсис\n",
    "x[..., 1]  # shape: (2, 3) - то же самое, что выше!\n",
    "# получение по списку индексов\n",
    "\n",
    "indices = torch.tensor([0, 1, 0])  # наш список индексов\n",
    "print(\"Исходный тензор:\")\n",
    "print(x)\n",
    "print(\"Получен по индексам:\")\n",
    "\n",
    "# получим срез по нашим индексам, применённым к различным размерностям:\n",
    "x.take_along_dim(indices.view(-1,  1,  1), dim=0)  # shape: (3, 3, 2)\n",
    "x.take_along_dim(indices.view( 1, -1,  1), dim=1)  # shape: (2, 3, 2)\n",
    "x.take_along_dim(indices.view( 1,  1, -1), dim=2)  # shape: (2, 3, 3)\n",
    "#x.take_along_dim(indices.view(-1, 1, 1), dim=2)  # shape: (3, 3, 2)\n",
    "#x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Broadcasting\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html#in-brief-tensor-broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые операции поддерживают `broadcast`, то есть размерности аргументов автоматически расширяются до нужного размера без копирования данных\n",
    "\n",
    "Общие правила, когда это работает:\n",
    "1. Все тензоры не пустые\n",
    "2. При сравнении размеров тензоров, начиная с последней:\n",
    "   1. Размерности совпадают, или\n",
    "   2. Одна из размерностей равна $1$, или\n",
    "   3. Размерность отсутствует в одном из тензоров\n",
    "\n",
    "\n",
    "Благодаря `broadcast` многие вещи получается описать лаконично."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно ли сложить тензоры `x` и `y` в примерах снизу?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=torch.empty(5,7,3)\n",
    "# y=torch.empty(5,7,3)\n",
    "\n",
    "# x=torch.empty((0,))\n",
    "# y=torch.empty(2,2)\n",
    "\n",
    "# x=torch.empty(5,3,4,1)\n",
    "# y=torch.empty(  3,1,1)\n",
    "\n",
    "# x=torch.empty(5,2,4,1)\n",
    "# y=torch.empty(  3,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Autograd в PyTorch\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Вычислительный граф и дифференцирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autograd` строит DAG (directed acyclic graph) из объектов `torch.autograd.Function`, листья - входные тензоры, корни - выходные тензоры. Проход по графу позволяет рассчитать градиенты по правилу производной сложной функции (chain rule).\n",
    "\n",
    "Прямой проход:\n",
    "- расчёт значения выходного тензора\n",
    "- построение графа и сохранение нужных для обратного прохода данных для каждой операции\n",
    "\n",
    "Обратный проход (вызов `.backward()` у корня графа):\n",
    "- расчёт градиентов и их накопление в артибуте `.grad` каждого тензора\n",
    "- распространение вычислений далее до листьев графа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим выражение $f(x, y) = x^2 + xy + (x + y)^2$ и построим его граф:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"expression_graph.png\" style=\"background:white\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем выражение для $f(x, y)$, задав начальные условия $x = 2.0, y = 2.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "a = x ** 2\n",
    "b = x*y\n",
    "c = x + y\n",
    "d = a + b\n",
    "e = c**2\n",
    "f = d + e\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grad_fn` означает, что `f` не просто отдельный тензор, а связан с вычислительным графом и соответствует операции `Add`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим backprop и убедимся, что градиенты рассчитаны правильно:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 2x + y + 2(x + y)$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = x + 2(x + y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.)\n",
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "f.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О чём стоит помнить:\n",
    "- Повторная попытка выполнить `backward()` приводит к ошибке, т.к. после первого вызова граф уничтожается для экономии ресурсов. Если мы хотим сохранить граф для повторного обратного прохода, делаем `f.backward(retain_graph=True)`\n",
    "- Градиенты в узлах графа не обнуляются автоматически, и при следующем вызове `backward` новое значение прибавится к старому\n",
    "- Если по какой-то причине мы не хотим считать градиенты, вычисление выражения можно обернуть в контекст ```with torch.no_grad():```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Отключение расчёта градиентов\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько способов:\n",
    "1. Изменить значение атрибута тензора `requires_grad` напрямую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = x + 1\n",
    "print(y.requires_grad)  # True\n",
    "x.requires_grad = False\n",
    "y = x + 1\n",
    "print(y.requires_grad)  # False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Использовать `torch.no_grad()` (как менеджер контекста или как декоратор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = x + 1\n",
    "print(x.requires_grad)  # True\n",
    "\n",
    "# локально отключаем трекинг градиентов\n",
    "with torch.no_grad():\n",
    "    y = x + 1\n",
    "print(y.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# декоратор\n",
    "def add_one(t: torch.Tensor) -> torch.Tensor:\n",
    "    return t + 1\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_two(t: torch.Tensor) -> torch.Tensor:\n",
    "    return t + 1\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = add_one(x)\n",
    "print(y.requires_grad)  # True\n",
    "z = add_two(x)\n",
    "print(z.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Получить копию тензора с помощью метода `.detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = x + 1\n",
    "print(x.requires_grad)  # True\n",
    "z = y.detach()\n",
    "print(z.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Пример: логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = \\sigma(w^T x + b)$\n",
    "\n",
    "$\\sigma(t) = \\frac{1}{1 + \\exp(-t)}$\n",
    "\n",
    "$\\text{CE}(y, \\hat{y}) = -y \\cdot \\log \\hat{y} - (1 - y) \\log (1 - \\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2493, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.ones(5)  # входной тензор\n",
    "y = torch.zeros(3)  # выходной тензор\n",
    "w = torch.randn(5, 3, requires_grad=True)  # параметр, хотим обновлять градиентным спуском\n",
    "b = torch.randn(3, requires_grad=True)  # параметр, хотим обновлять градиентным спуском\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)  # функция потерь, хотим минимизировать\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x138989510>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x13898a3e0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" style=\"background:white\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем цикл для поиска параметров, минимизирующих функцию ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые распространённые ошибки:\n",
    "\n",
    "1. Обновление параметра на месте вне контекста `torch.no_grad()` вызовет ошибку. PyTorch такое явно запрещает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m f \u001b[39m=\u001b[39m w \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      4\u001b[0m f\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m----> 5\u001b[0m w \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mgrad\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "#x = torch.tensor(1.0)\n",
    "f = w + 1\n",
    "f.backward()\n",
    "w -= w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Обновление параметра через присваивание приводит к тому, что параметр больше не является листом графа, в графе образовался цикл, и следующая итерация приведёт к ошибке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z3/n11m8hqx40s1trg9fk01s2qh0000gp/T/ipykernel_21890/1172668910.py:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343686130/work/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  w = w - w.grad  # здесь w.grad is None!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# вторая итерация - оказывается, что не ок\u001b[39;00m\n\u001b[1;32m     10\u001b[0m f\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 11\u001b[0m w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39;49m w\u001b[39m.\u001b[39;49mgrad  \u001b[39m# здесь w.grad is None!\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "#x = torch.tensor(1.0)\n",
    "f = w + 1\n",
    "# первая итерация - всё как будто ок\n",
    "f.backward()\n",
    "w = w - w.grad  # на этом моменте `w` - больше не лист графа\n",
    "f = w + 1\n",
    "\n",
    "# вторая итерация - оказывается, что не ок\n",
    "f.backward()\n",
    "w = w - w.grad  # здесь w.grad is None!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Внутри контекста `torch.no_grad()` параметр обновляем не на месте, а через переназначение, после этого он более не ожидает градиентов, всё ломается при вызове `.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m f \u001b[39m=\u001b[39m w \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39m# вторая итерация\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m f\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-course/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-course/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "f = w + 1\n",
    "# первая итерация\n",
    "f.backward()\n",
    "with torch.no_grad():\n",
    "    w = w - w.grad  # упс, для w теперь requires_grad = False!\n",
    "f = w + 1\n",
    "\n",
    "# вторая итерация\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Класс `torch.autograd.Function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда может пригодиться:\n",
    "1. Добавление недифференцируемых вычислений\n",
    "2. Добавление операций, реализованный вне PyTorch (NumPy, SciPy, etc.)\n",
    "3. Более эффективное использование ресурсов (комбинирование операций, обёртка над реализацией на `C++`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulConstant(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor, constant):\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        tensor, constant = inputs\n",
    "        ctx.constant = constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor1, tensor2):\n",
    "        return tensor1 * tensor2\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        tensor1, tensor2 = inputs\n",
    "        ctx.tensor1 = tensor1\n",
    "        ctx.tensor2 = tensor2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        return grad_output * ctx.tensor2, grad_output * ctx.tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  tensor(8., grad_fn=<MulConstantBackward>)\n",
      "Gradient for x:  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "mul_const = MulConstant.apply\n",
    "res = mul_const(x, 4)\n",
    "print(\"Result: \", res)\n",
    "res.backward()\n",
    "print(\"Gradient for x: \", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  tensor(6., grad_fn=<MulBackward>)\n",
      "Gradient for x:  tensor(3.)\n",
      "Gradient for y:  tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "mul = Mul.apply\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "res = mul(x, y)\n",
    "print(\"Result: \", res)\n",
    "res.backward()\n",
    "print(\"Gradient for x: \", x.grad)\n",
    "print(\"Gradient for y: \", y.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Упражнения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Функция Power\n",
    "Используя сложение и умножение, реализуйте возведение в целочисленную степень FloatTensor как функцию autograd (т.е. наследника `torch.autograd.Function`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class Power(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor, p):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(torch.all(Power.apply(torch.tensor([1, 2, 3]), 0) == torch.tensor([1, 1, 1])))\n",
    "assert(torch.all(Power.apply(torch.tensor([1, 2, 3]), 2) == torch.tensor([1, 4, 9])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Многочлен\n",
    "Найдите корень (он один) заданного полинома (очень хорошего!) с точностью до пяти знаков после запятой:\n",
    "1. Используя бинарный поиск https://en.wikipedia.org/wiki/Binary_search_algorithm\n",
    "2. Используя метод Ньютона https://en.wikipedia.org/wiki/Newton%27s_method\n",
    "   \n",
    "   Задаётся начальное приближение вблизи предположительного корня, после чего строится касательная к графику исследуемой функции в точке приближения, для которой находится пересечение с осью абсцисс. Эта точка берётся в качестве следующего приближения. И так далее, пока не будет достигнута необходимая точность.\n",
    "   \n",
    "   (hint: для вычисления производных используйте метод `backward()`)\n",
    "   \n",
    "   $x_{n+1} = x_{n} - \\frac{f(x_n)}{f'(x_n)}$\n",
    "\n",
    "Сравните скорость методов с помощью `%%timeit`, т.е. оцените, какой из них найдёт ответ быстрее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "Polynomial = Callable[[torch.FloatTensor], torch.FloatTensor]\n",
    "\n",
    "def poly(x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    return x ** 7 + 5 * x ** 3 + 17 * x - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_search_find_zero(poly: Polynomial) -> torch.FloatTensor:\n",
    "  \"\"\"Функция для бинарного поиска\"\"\"\n",
    "  ...\n",
    "  return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_find_zero(poly: Polynomial) -> torch.FloatTensor:\n",
    "    \"\"\"Функция для метода Ньютона\"\"\"\n",
    "\n",
    "    # первое приближение (не забываем про то, что понадобится градиент!)\n",
    "    x = ...\n",
    "\n",
    "    # останавливаемся, если значение функции достаточно близко к нулю\n",
    "    tol = 10 ** -5\n",
    "\n",
    "    # значение \n",
    "    val = ...\n",
    "\n",
    "    # цикл обновления\n",
    "    while ...:  # когда останавливаемся?\n",
    "        # получаем градиент, обновляем значение x, оцениваем f(x)\n",
    "        # hint: нужны ли нам градиенты, когда мы обновляем x?\n",
    "        # hint: не забываем про обнуление градиента с прошлых шагов\n",
    "        ...\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = newton_find_zero(poly)\n",
    "print(x)\n",
    "print(poly(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = newton_find_zero(poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Что почитать / посмотреть\n",
    "1. [Backpropagation: анимированное изложение](https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll)\n",
    "2. [You should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b) от Andrej Karpathy\n",
    "3. [Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
