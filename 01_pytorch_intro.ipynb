{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Организационная информация\n",
    "\n",
    "\n",
    "Основная коммуникация: Telegram `DL - СПбГУ - 2023`. Если вас там нет, напишите `@snikolenko`, чтобы он вас туда добавил.\n",
    "\n",
    "Материалы и домашние работы: `emkn`, GitHub: https://github.com/norsage/dl-mkn.git\n",
    "\n",
    "Аттестация: по результатам результатов решений нескольких прикладных задач, где потребуется побить бейзлайн\n",
    "\n",
    "Упражнения после занятия: опциональны, но очень полезны для развития интуиции\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. План на сегодня: знакомство с PyTorch\n",
    "\n",
    "1. Тензоры и операции над ними\n",
    "2. Граф вычислений и автоматическое дифференцирование\n",
    "3. Реализация новых операций с помощью `torch.autograd.Function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Подготовка окружения\n",
    "\n",
    "1) Локально с помощью conda\n",
    "   \n",
    "   (можно установить по инструкции https://docs.conda.io/projects/miniconda/en/latest/index.html#quick-command-line-install)\n",
    "   ```bash\n",
    "   # создаём окружение dl-course\n",
    "   conda create -n dl-course -y python=3.10\n",
    "   # активируем окружение\n",
    "   conda activate dl-course\n",
    "   # устанавливаем Jupyter\n",
    "   conda install jupyter notebook -c conda-forge\n",
    "\n",
    "   # Устанавливаем pytorch (https://pytorch.org/get-started/locally/)\n",
    "   # Пример для Linux / Windows с поддержкой GPU:\n",
    "   conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "   # Для MacOS с ускорением MPS:\n",
    "   # conda install pytorch::pytorch torchvision torchaudio -c pytorch\n",
    "   ```\n",
    "\n",
    "   Запустить ноутбук можно как в браузере:\n",
    "   \n",
    "   `jupyter notebook 01_pytorch_intro.ipynb`\n",
    "\n",
    "   так и в VSCode, просто открыв файл и выбрав кернел для исполнения в правом верхнем углу (потребуется поставить расширения `Python` и `Jupyter`)\n",
    "2) Используем Сolab https://colab.research.google.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PyTorch basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Тензоры, способы создания и атрибуты\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать тензор можно многими способами:\n",
    "1. Напрямую из объектов\n",
    "2. Из массивов `numpy`\n",
    "3. Из других тензоров\n",
    "4. С константными и случайными значениями\n",
    "5. Используя специальные функции для особых случаев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные атрибуты: ранг (`dim`), размерности (`shape`), тип значений (`type`), место размещения (`device`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Простые операции, функции, линейная алгебра\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Различных операций сотни, можно выделить несколько групп по назначению:\n",
    "- создание (`linspace`, `arange`, `full`)\n",
    "- индексация, срезы, объединения (`argwhere`, `cat`, `stack`, `tile`, `gather`)\n",
    "- генерация случайных величин (`normal`, `bernoulli`, `randperm`)\n",
    "- сериализация / десериализация (`save`, `load`)\n",
    "- математические операции (`log`, `deg2rad`, `clamp`, `sigmoid`)\n",
    "- агрегирование (`sum`, `mean`, `argmax`, `quantile`)\n",
    "- линейная алгебра (`torch.linalg.*`)\n",
    "- FFT (`torch.fft.*`)\n",
    "- обработка сигналов (`torch.signal.*`)\n",
    "- нейронные сети (`torch.nn.*`)\n",
    "- ...\n",
    "\n",
    "Многие операции также реализованы как методы класса `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Broadcasting\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html#in-brief-tensor-broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые операции поддерживают `broadcast`, то есть размерности аргументов автоматически расширяются до нужного размера без копирования данных\n",
    "\n",
    "Общие правила, когда это работает:\n",
    "1. Все тензоры не пустые\n",
    "2. При сравнении размеров тензоров, начиная с последней:\n",
    "   1. Размерности совпадают, или\n",
    "   2. Одна из размерностей равна $1$, или\n",
    "   3. Размерность отсутствует в одном из тензоров\n",
    "\n",
    "\n",
    "Благодаря `broadcast` многие вещи получается описать лаконично."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=torch.empty(5,7,3)\n",
    "# y=torch.empty(5,7,3)\n",
    "\n",
    "# x=torch.empty((0,))\n",
    "# y=torch.empty(2,2)\n",
    "\n",
    "# x=torch.empty(5,3,4,1)\n",
    "# y=torch.empty(  3,1,1)\n",
    "\n",
    "# x=torch.empty(5,2,4,1)\n",
    "# y=torch.empty(  3,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Autograd в PyTorch\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Вычислительный граф и дифференцирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autograd` строит DAG (directed acyclic graph) из объектов `torch.autograd.Function`, листья - входные тензоры, корни - выходные тензоры. Проход по графу позволяет рассчитать градиенты по правилу производной сложной функции (chain rule).\n",
    "\n",
    "Прямой проход:\n",
    "- расчёт значения выходного тензора\n",
    "- построение графа и сохранение нужных для обратного прохода данных для каждой операции\n",
    "\n",
    "Обратный проход (вызов `.backward()` у корня графа):\n",
    "- расчёт градиентов и их накопление в артибуте `.grad` каждого тензора\n",
    "- распространение вычислений далее до листьев графа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим выражение $f(x, y) = x^2 + xy + (x + y)^2$ и построим его граф:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"expression_graph.png\" style=\"background:white\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем выражение для $f(x, y)$, задав начальные условия $x = 2.0, y = 2.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grad_fn` означает, что `f` не просто отдельный тензор, а связан с вычислительным графом и соответствует операции `Add`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим backprop и убедимся, что градиенты рассчитаны правильно:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 2x + y + 2(x + y)$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = x + 2(x + y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О чём стоит помнить:\n",
    "- Повторная попытка выполнить `backward()` приводит к ошибке, т.к. после первого вызова граф уничтожается для экономии ресурсов. Если мы хотим сохранить граф для повторного обратного прохода, делаем `f.backward(retain_graph=True)`\n",
    "- Градиенты в узлах графа не обнуляются автоматически, и при следующем вызове `backward` новое значение прибавится к старому\n",
    "- Если по какой-то причине мы не хотим считать градиенты, вычисление выражения можно обернуть в контекст ```with torch.no_grad():```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Отключение расчёта градиентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько способов:\n",
    "1. Изменить значение атрибута тензора `requires_grad` напрямую\n",
    "2. Использовать `torch.no_grad()` (как менеджер контекста или как декоратор)\n",
    "3. Получить копию тензора с помощью метода `.detach()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Пример: логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = \\sigma(w^T x + b)$\n",
    "\n",
    "$\\sigma(t) = \\frac{1}{1 + \\exp(-t)}$\n",
    "\n",
    "$\\text{CE}(y, \\hat{y}) = -y \\cdot \\log \\hat{y} - (1 - y) \\log (1 - \\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" style=\"background:white\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем цикл для поиска параметров, минимизирующих функцию ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Класс `torch.autograd.Function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда может пригодиться:\n",
    "1. Добавление недифференцируемых вычислений\n",
    "2. Добавление операций, реализованный вне PyTorch (NumPy, SciPy, etc.)\n",
    "3. Более эффективное использование ресурсов (комбинирование операций, обёртка над реализацией на `C++`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulConstant(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor, constant):\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        tensor, constant = inputs\n",
    "        ctx.constant = constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor1, tensor2):\n",
    "        return tensor1 * tensor2\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        tensor1, tensor2 = inputs\n",
    "        ctx.tensor1 = tensor1\n",
    "        ctx.tensor2 = tensor2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        return grad_output * ctx.tensor1, grad_output * ctx.tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Упражнения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Функция Power\n",
    "Используя сложение и умножение, реализуйте возведение в целочисленную степень FloatTensor как функцию autograd (т.е. наследника `torch.autograd.Function`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class Power(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor, p):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(torch.all(Power.apply(torch.tensor([1, 2, 3]), 0) == torch.tensor([1, 1, 1])))\n",
    "assert(torch.all(Power.apply(torch.tensor([1, 2, 3]), 2) == torch.tensor([1, 4, 9])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Многочлен\n",
    "Найдите корень (он один) заданного полинома (очень хорошего!) с точностью до пяти знаков после запятой:\n",
    "1. Используя бинарный поиск https://en.wikipedia.org/wiki/Binary_search_algorithm\n",
    "2. Используя метод Ньютона https://en.wikipedia.org/wiki/Newton%27s_method\n",
    "   \n",
    "   Задаётся начальное приближение вблизи предположительного корня, после чего строится касательная к графику исследуемой функции в точке приближения, для которой находится пересечение с осью абсцисс. Эта точка берётся в качестве следующего приближения. И так далее, пока не будет достигнута необходимая точность.\n",
    "   \n",
    "   (hint: для вычисления производных используйте метод `backward()`)\n",
    "   \n",
    "   $x_{n+1} = x_{n} - \\frac{f(x_n)}{f'(x_n)}$\n",
    "\n",
    "Сравните скорость методов с помощью `%%timeit`, т.е. оцените, какой из них найдёт ответ быстрее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "Polynomial = Callable[[torch.FloatTensor], torch.FloatTensor]\n",
    "\n",
    "def poly(x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    return x ** 7 + 5 * x ** 3 + 17 * x - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_search_find_zero(poly: Polynomial) -> torch.FloatTensor:\n",
    "  \"\"\"Функция для бинарного поиска\"\"\"\n",
    "  ...\n",
    "  return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_find_zero(poly: Polynomial) -> torch.FloatTensor:\n",
    "    \"\"\"Функция для метода Ньютона\"\"\"\n",
    "\n",
    "    # первое приближение (не забываем про то, что понадобится градиент!)\n",
    "    x = ...\n",
    "\n",
    "    # останавливаемся, если значение функции достаточно близко к нулю\n",
    "    tol = 10 ** -5\n",
    "\n",
    "    # значение \n",
    "    val = ...\n",
    "\n",
    "    # цикл обновления\n",
    "    while ...:  # когда останавливаемся?\n",
    "        # получаем градиент, обновляем значение x, оцениваем f(x)\n",
    "        # hint: нужны ли нам градиенты, когда мы обновляем x?\n",
    "        # hint: не забываем про обнуление градиента с прошлых шагов\n",
    "        ...\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = newton_find_zero(poly)\n",
    "print(x)\n",
    "print(poly(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = newton_find_zero(poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Что почитать / посмотреть\n",
    "1. [Backpropagation: анимированное изложение](https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll)\n",
    "2. [You should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b) от Andrej Karpathy\n",
    "3. [Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
