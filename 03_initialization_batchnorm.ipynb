{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как не потерять градиент: функции активации, инициализация, нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "План на сегодня: разбираем возможные проблемы при обучении\n",
    "1. Инициализация\n",
    "   1. Диагностика проблем:\n",
    "      - начальное распределение выхода\n",
    "      - значения промежуточных активаций и нелинейности с насыщением\n",
    "   2. Настраиваем инициализацию весов\n",
    "2. Нормализация по батчам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Впереди нас ждут разные сложные архитектуры, но перед этим задержимся подольше на MLP, чтобы получить большьше интуитивного понимания об активациях и градиентах в нейронных сетях в процессе обучения\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Подготовим данные, модель и функции для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1. Загружаем MNIST и создаём загрузчики данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    'data', \n",
    "    train=True, \n",
    "    download=True,    \n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    'data', \n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2. Задаём архитектуру модели\n",
    "\n",
    "Такая же, как в прошлый раз, но параметры задаём явно и инициализируем значениями из $\\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Parameter(torch.randn((input_dim, hidden_dim)), requires_grad=True)\n",
    "        self.b1 = nn.Parameter(torch.randn(hidden_dim), requires_grad=True)\n",
    "        self.w2 = nn.Parameter(torch.randn((hidden_dim, output_dim)), requires_grad=True)\n",
    "        self.b2 = nn.Parameter(torch.randn(output_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        h = x.flatten(1) @ self.w1 + self.b1\n",
    "        h_act = F.tanh(h)\n",
    "        logits = h_act @ self.w2 + self.b2\n",
    "        # помимо логитов, вернём ещё промежуточные активации - они нам понадобятся\n",
    "        return logits, h_act, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3. Определим функции обучения\n",
    "\n",
    "Всё как обычно, но\n",
    "1. градиенты обновляем вручную\n",
    "2. эпоху ограничиваем сотней батчей - просто для скорости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch: tuple[torch.Tensor, torch.Tensor], model: MLP, lr: float = 0.01) -> torch.Tensor:\n",
    "    # прогоняем батч через модель\n",
    "    x, y = batch\n",
    "    logits, *_ = model(x)\n",
    "    # оцениваем значение ошибки\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # обновляем параметры\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.data -= lr * param.grad\n",
    "        param.grad = None\n",
    "    # возвращаем значение функции ошибки для логирования\n",
    "    return loss\n",
    "\n",
    "def train_epoch(dataloader: DataLoader, model: MLP, lr: float = 0.01, max_batches: int = 100) -> Tensor:\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        loss = training_step(batch, model, lr)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    return torch.tensor(loss_values).mean()\n",
    "\n",
    "def test_epoch(dataloader: DataLoader, model: MLP, max_batches: int = 100) -> Tensor:\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            logits, *_ = model(x)\n",
    "        # оцениваем значение ошибки\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    return torch.tensor(loss_values).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Начальное распределение классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим обучение на несколько эпох и понаблюдаем за изменением ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "x, y = next(iter(train_loader))\n",
    "input_dim = 784\n",
    "hidden_dim = 128\n",
    "output_dim = len(train_dataset.classes)\n",
    "# создадим модель и выведем значение ошибки после инициализации\n",
    "model = MLP(input_dim, hidden_dim, output_dim)\n",
    "logits, h_act, h = model(x)\n",
    "loss = F.cross_entropy(logits, y)\n",
    "print(f\"Initial loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batches_per_epoch = 100\n",
    "for i in range(n_epochs):\n",
    "    loss = train_epoch(train_loader, model, lr=0.1, max_batches=batches_per_epoch)\n",
    "    print(f\"Epoch {i} loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"Test loss: {test_epoch(test_loader, model, max_batches=batches_per_epoch):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы стартовали с очень высокого значения ошибки, но уже к третьей эпохе модель сошлась к значению около 2, после чего ошибка уже изменялась понемногу. Почему так?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель очень неоптимально сконфигурирована на этапе инициализации.\n",
    "Начальный лосс очень далёк от ожидаемого - значит, инициализация точно плохая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А какое ожидаемое значение ошибки?\n",
    "\n",
    "$$CrossEntropy(\\hat{y}, y) = - \\sum_i y_i \\log \\hat{y}_i + (1 - y_i) \\log(1 - \\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что же пошло не так?\n",
    "\n",
    "Посмотрим, что происходит с ошибкой в зависимости от значений логитов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посмотрим на наши логиты из модели. Что можно сказать о распределении над классами? Что это говорит о нашей инициализации?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как нам добиться близости к нулю для логитов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Чем занималась оптимизация первое время? Только шкалированием логитов -->\n",
    "\n",
    "Изменилось ли значение ошибки, когда мы убрали \"лёгкую часть\" задачи?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Значения промежуточных активаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Rahul-Jayawardana/publication/350567223/figure/fig3/AS:1007855343767554@1617302847631/Fig-3-The-basic-activation-functions-of-the-neural-networksNeural-Networks.jpg\" style=\"background:white\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей модели мы используем нелинейность `tanh` между линейными слоями.\n",
    "\n",
    "$$\\tanh z = \\frac{e^z - e^{-z}}{e^z - e^{-z}}$$\n",
    "\n",
    "$$\\frac{d(\\tanh z)}{dz} = 1 - \\tanh^2 z$$\n",
    "\n",
    "Что происходит с градиентами при значениях активации близких к $0$? Близких к $1$ и $-1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на активации после нелинейности, применённой на выходы из первого слоя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Это тот самый \"vanishing gradient\", о котором мы ещё услышим позже, когда будем говорить про рекуррентные сети -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на масштаб проблемы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 20))\n",
    "# plt.imshow(h_act.abs() > 0.99, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент будет уничтожен везде, где у нас белый пиксел.\n",
    "\n",
    "А если найдётся целиком белый столбец?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. А как правильно?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что происходит с распределением значений, когда мы перемножаем две матрицы, инициализированные стандартным нормальным распределением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200)\n",
    "y =  x @ w\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "ax1.hist(x.flatten().tolist(), 20, density=True)\n",
    "ax2.hist(y.flatten().tolist(), 20, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение расползлось, как исправить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**: какое распределение имеет $Y = X \\cdot W$, где $X \\in \\mathbb{R}^{m \\times k}$, $W \\in \\mathbb{R}^{k \\times n}$ и $X_{ij}, W_{ij} \\sim \\mathcal{N}(0, 1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике этого достаточно, но если хочется предельной точности: https://pytorch.org/docs/stable/nn.init.html\n",
    "   $$\\text{std} = \\frac{\\text{gain}(f_{act})}{\\sqrt{\\text{fan mode}}}$$\n",
    "\n",
    "Пример статьи с обоснованием для ReLU и PReLU:\n",
    "\n",
    "[Kaiming He et al. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, есть ли проблемы с встроенным слоем `torch.nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        h = self.l1(x.flatten(1))\n",
    "        h_act = F.tanh(h)\n",
    "        logits = self.l2(h_act)\n",
    "        return logits, h_act, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "x, y = next(iter(train_loader))\n",
    "# создадим модель и выведем значение ошибки после инициализации\n",
    "model = MLP(input_dim, hidden_dim, output_dim)\n",
    "logits, h_act, h = model(x)\n",
    "loss = F.cross_entropy(logits, y)\n",
    "print(f\"Initial loss: {loss:.4f}\")\n",
    "\n",
    "n_epochs = 10\n",
    "batches_per_epoch = 100\n",
    "for i in range(n_epochs):\n",
    "    loss = train_epoch(train_loader, model, lr=0.1, max_batches=batches_per_epoch)\n",
    "    print(f\"Epoch {i} loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"Test loss: {test_epoch(test_loader, model, max_batches=batches_per_epoch):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явных проблем нет: инициализация по умолчанию настроена хорошо (и мы можем посмотреть, как именно)\n",
    "\n",
    "Попробуем сделать ещё лучше?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Резюме\n",
    "1. Проблемы с внутренними активациями возникают не только на инициализации, но и в процессе оптимизации - один большой шаг в неправильную сторону может убить нейрон (в случае с ReLU - навсегда)\n",
    "2. В маленьких моделях проблемы инициализации не так критичны - сеть в итоге обучится, просто потребуется больше времени.\n",
    "3. Чем глубже сеть (больше слоёв) - тем больше проблем\n",
    "4. В последние годы добавилось много трюков, которые сделали инициализацию менее критичной:\n",
    "   1. residual connections\n",
    "   2. normalization layers (batch, layer, group)\n",
    "   3. лучшие оптимизаторы (RMSProp, Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ioffe, Szegedy (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотим, чтобы активации не были ни слишком малыми, ни слишком большими, и были близки к стандартному нормальному распределению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея: а давайте просто возьмём активации в батче, вычтем среднее и поделим на стандартное отклонение!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://kharshit.github.io/img/batch_normalization.png\" style=\"background:white\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какой ценой? Теперь значение активаций для одного примера уже не детерминировано - оно зависит также от других примеров в батче, который формируется случайным образом. \n",
    "\n",
    "Внезапно, это не так уж плохо: мы учим сеть быть устойчивой к небольшим вариациям входных данных.\n",
    "\n",
    "Но как теперь получать предсказания для одного изолированного примера?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
