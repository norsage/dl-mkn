{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer и multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "План:\n",
    "- посмотреть, как работает трансформер T5 на задаче первода с немецкого на английский с прошлой недели\n",
    "- рассмотреть внутреннее устройство трансформера:\n",
    "  - LayerNorm\n",
    "  - Multi-head attention\n",
    "  - Блоки энкодера и декодера\n",
    "- попробовать обучить не хуже библиотечного"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Забегая вперёд: T5\n",
    "\n",
    "paper: https://arxiv.org/abs/1910.10683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from torch import nn, Tensor\n",
    "import math\n",
    "from typing import cast\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1153f1a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Датасет с прошлой практики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "\n",
    "\n",
    "class Multi30kDataset(L.LightningDataModule):\n",
    "    train_dataset: Dataset\n",
    "    test_dataset: Dataset\n",
    "    tokenizer: T5Tokenizer\n",
    "\n",
    "    def __init__(self, maxlen: int = 0, batch_size: int = 32) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(\n",
    "            \"t5-small\", padding_size=\"right\", bos_token=\"</b>\", legacy=False\n",
    "        )\n",
    "        if not Path(\"bentrevett/multi30k-train\").is_dir():\n",
    "            self.train_dataset = load_dataset(\"bentrevett/multi30k\", split=\"train\")\n",
    "            self.test_dataset = load_dataset(\"bentrevett/multi30k\", split=\"test\")\n",
    "            self.train_dataset.save_to_disk(\"bentrevett/multi30k-train\")\n",
    "            self.test_dataset.save_to_disk(\"bentrevett/multi30k-test\")\n",
    "        else:\n",
    "            self.train_dataset = load_from_disk(\"bentrevett/multi30k-train\")\n",
    "            self.test_dataset = load_from_disk(\"bentrevett/multi30k-test\")\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        if maxlen > 0:\n",
    "            self.train_dataset = self.filter_dataset(self.train_dataset, maxlen)\n",
    "            self.test_dataset = self.filter_dataset(self.test_dataset, maxlen)\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch: list[tuple[str, str]]) -> tuple[Tensor, Tensor]:\n",
    "        prompt = self.tokenizer.bos_token\n",
    "        inputs, targets = zip(*[(pair[\"de\"], prompt + pair[\"en\"]) for pair in batch])\n",
    "        encoded_batch = self.tokenizer(\n",
    "            inputs, text_target=targets, padding=\"longest\", return_tensors=\"pt\"\n",
    "        )\n",
    "        return encoded_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_dataset(dataset, maxlen: int) -> list[dict[str, str]]:\n",
    "        return [\n",
    "            dataset[i]\n",
    "            for i in range(len(dataset))\n",
    "            if len(dataset[i][\"en\"].split(\" \")) <= maxlen\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "multi30k = Multi30kDataset(maxlen=7, batch_size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как быстро обучится T5 - одна из версий трансформера, без каких-либо рекуррентных блоков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# t5.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обернём в `LightningModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "\n",
    "\n",
    "class Seq2Seq(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        lr: float = 0.01,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lr = lr\n",
    "\n",
    "    def training_step(self, batch: dict[str, Tensor], batch_idx: int) -> STEP_OUTPUT:\n",
    "        if isinstance(self.model, T5ForConditionalGeneration):\n",
    "            model_outputs = self.model.forward(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n",
    "            loss = model_outputs.loss\n",
    "        else:\n",
    "            logits = self.model.forward(\n",
    "                batch[\"input_ids\"], batch[\"labels\"]\n",
    "            )\n",
    "            loss = F.cross_entropy(\n",
    "                logits[:, :-1].reshape(-1, len(self.tokenizer)),\n",
    "                batch[\"labels\"][:, 1:].flatten(),\n",
    "            )\n",
    "        self.log(\"loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/sarapulov/miniconda3/envs/dl-course/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/Users/sarapulov/miniconda3/envs/dl-course/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /Users/sarapulov/projects/dl-mkn/workshops/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n",
      "/Users/sarapulov/miniconda3/envs/dl-course/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc8c5a7400747e3b9983463c48661be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(accelerator=\"cpu\", max_epochs=1, limit_train_batches=200, logger=False)\n",
    "seq2seq = Seq2Seq(t5, multi30k.tokenizer, lr=0.001)\n",
    "trainer.fit(model=seq2seq, train_dataloaders=multi30k.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на пример перевода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarapulov/miniconda3/envs/dl-course/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deutsch: Ein Typ arbeitet an einem Gebäude.\n",
      "English: A guy works on a building.\n",
      "Translation: A man working on a building.\n",
      "\n",
      "Deutsch: Drei Leute sitzen in einer Höhle.\n",
      "English: Three people sit in a cave.\n",
      "Translation: Three people sitting in a cave.\n",
      "\n",
      "Deutsch: Leute, die vor einem Gebäude stehen.\n",
      "English: People standing outside of a building.\n",
      "Translation: People are standing in front of a building.\n",
      "\n",
      "Deutsch: Ein Mann schneidet ste von Bäumen.\n",
      "English: A man cutting branches of trees.\n",
      "Translation: Man schneidet trees.\n",
      "\n",
      "Deutsch: Ein Kind planscht im Wasser.\n",
      "English: A child is splashing in the water\n",
      "Translation: A child plans out of water.\n",
      "\n",
      "Deutsch: Eine schöne Frau spielt auf einer Harfe.\n",
      "English: A pretty woman plays a harpsichord.\n",
      "Translation: A nice woman playing on a wall.\n",
      "\n",
      "Deutsch: Leute sitzen in einem Zug.\n",
      "English: People sit inside a train.\n",
      "Translation: People sitting in a train.\n",
      "\n",
      "Deutsch: Ein kleines Kind kocht mit einer anderen Person.\n",
      "English: A toddler is cooking with another person.\n",
      "Translation: A little boy is cooking with another person.\n",
      "\n",
      "Deutsch: Ein Mann bereitet am Herd Essen zu.\n",
      "English: A man cooking food on the stove.\n",
      "Translation: A man is eating food.\n",
      "\n",
      "Deutsch: Ein am Strand geparktes Auto.\n",
      "English: A car parked at the beach.\n",
      "Translation: An adolescent on the beach.\n",
      "\n",
      "Deutsch: Zwei Männer in Schwarz in einer Stadt\n",
      "English: Two men wearing black in a city\n",
      "Translation: Two men in black in a city.\n",
      "\n",
      "Deutsch: Ein junges Mädchen schwimmt in einem Pool\n",
      "English: A young girl swimming in a pool\n",
      "Translation: A young girl schwimming in a pool.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(multi30k.test_dataloader()))\n",
    "preds = t5.generate(batch[\"input_ids\"])\n",
    "\n",
    "source, target, preds = map(\n",
    "    lambda x: multi30k.tokenizer.batch_decode(x, skip_special_tokens=True),\n",
    "    (batch[\"input_ids\"], batch[\"labels\"], preds),\n",
    ")\n",
    "\n",
    "for src, tgt, pred in zip(source, target, preds):\n",
    "    print(f\"Deutsch: {src}\")\n",
    "    print(f\"English: {tgt}\")\n",
    "    print(f\"Translation: {pred}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. А теперь по порядку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер:\n",
    "- начинаем с начальных эмбеддингов слов и их позиций (embedding / positional encoding)\n",
    "- внутри блока энкодера каждый токен обменивается информацией с каждым, вес определяется механизмом внимания (self-attention)\n",
    "- все токены обрабатываются параллельно\n",
    "\n",
    "Декодер:\n",
    "- начинаем с эмбеддинга текущего слова и его позиции (embedding / positional encoding)\n",
    "- обновляем его механизмом внимания, но смотрим только на предыдущие слова (*masked self-attention*)\n",
    "- обновляем эмбеддинг текущего токена вниманием на выход энкодера (*cross-attention*)\n",
    "- движемся от начала предложения к концу последовательно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/transformer/transformer_original.gif\" style=\"background:white\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на архитектуру в деталях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/transformer.png\" style=\"background:white\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input / Output Embedding - уже знакомый `nn.Embedding` слой\n",
    "- Positional Encoding - было в практике VAE & DDPM (sinusoidal encoding), сегодня тоже возьмём `nn.Embedding`\n",
    "- **Multi-Head [Masked] Attention** - сердце трансформера, будем детально разбирать\n",
    "- Add & Norm - skip connection (знакомое) и **layer normalization** (посмотрим на отличие от batchnorm)\n",
    "- Feed Forward - обычный перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. LayerNorm\n",
    "\n",
    "$\\begin{aligned}\n",
    "y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
    "\\end{aligned}$\n",
    "\n",
    "Но среднее и дисперсию считаем по-разному:\n",
    "- BatchNorm: вдоль батча и пространственных (или временных) размерностей независимо для каждого канала\n",
    "- LayerNorm: вдоль всех каналов отдельного примера\n",
    "<!-- - BatchNorm: across batch size and spatial dimensions independently for each feature channel\n",
    "- LayerNorm: across all channels for each individual sample -->\n",
    "\n",
    "<!-- <img src=\"https://i.stack.imgur.com/fAowJ.png\" style=\"background:white\" width=\"800\"/> -->\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/7ZO1R.png\" style=\"background:white\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, sentence_length, embedding_dim = 3, 3, 5\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "var, mean = torch.var_mean(embedding, dim=[2], unbiased=False, keepdim=True)\n",
    "\n",
    "layer_norm = nn.LayerNorm(embedding_dim, elementwise_affine=False)\n",
    "assert torch.allclose(\n",
    "    layer_norm(embedding), (embedding - mean) / torch.sqrt(var), rtol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Multi-head attention - главный компонент self-attention и cross-attention\n",
    "\n",
    "Основная идея:\n",
    "- На основе входных эмбеддингов рассчитываем векторы запросов $Q$, ключей $K$ и значений $V$\n",
    "- Дополнительно предоставляется **маска внимания**: каким парам токенов разрешено смотреть друг на друга\n",
    "- Мера релевантности каждого ключа нашему запросу - их **скалярное произведение**\n",
    "- Новое [промежуточное] значение эмбеддинга - взвешенная по релевантности сумма значений\n",
    "- Итоговый эмбеддинг - линейная проекция нескольких конкатенированных голов\n",
    "\n",
    "Примечание: источником ключей и значений могут быть как общие данные (*self-attention*), так и различные (*cross-attention*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/A-X/publication/334427742/figure/fig3/AS:987861692211201@1612535989364/Structure-of-multi-head-attention-layer.ppm\" style=\"background:white\" height=\"350\"/>\n",
    "\n",
    "<img src=\"https://blog.sailor.plus/deep-learning/images/1613723693323.png\" style=\"background:white\" height=\"350\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim: int, n_heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # query, key and value\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        # out projection\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "    def forward(\n",
    "        self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor | None = None\n",
    "    ) -> tuple[Tensor, Tensor]:\n",
    "        B, T, C = query.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        # 1. получаем значения Q, K, V из линейных слоёв\n",
    "        Q = self.fc_q.forward(query)  # B x T x C\n",
    "        K = self.fc_k.forward(key)  # B x T x C\n",
    "        V = self.fc_v.forward(value)  # B x T x C\n",
    "\n",
    "        # 2. разбиваем результат на \"головы\" и выносим их размерность вперёд\n",
    "        Q = Q.view(B, -1, H, C // H).permute(\n",
    "            0, 2, 1, 3\n",
    "        )  # B x T x C -> B x H x Lq x (C / H)\n",
    "        K = K.view(B, -1, H, C // H).permute(\n",
    "            0, 2, 1, 3\n",
    "        )  # B x T x C -> B x H x Lk x (C / H)\n",
    "        V = V.view(B, -1, H, C // H).permute(\n",
    "            0, 2, 1, 3\n",
    "        )  # B x T x C -> B x H x Lk x (C / H)\n",
    "\n",
    "        # 3. рассчитываем матрицу внимания как скалярное произведение\n",
    "        # между всеми парами токенов из Q и K для всех голов внимания\n",
    "        scale = math.sqrt(C // H)\n",
    "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / scale  # B x H x Lq x Lk\n",
    "\n",
    "        # 4. получаем веса внимания для разрешённых токенов\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention = torch.softmax(attention, dim=-1)  # B x H x Lq x Lk\n",
    "\n",
    "        # 5. Получаем значения для всех голов внимания\n",
    "        h = torch.matmul(attention, V)  # B x H x Lq x C / H\n",
    "\n",
    "        # 6. конкатенируем головы и делаем финальную проекцию\n",
    "        h = h.permute(0, 2, 1, 3).contiguous()  # B x Lq x H x (C / H)\n",
    "        h = self.fc_o(h.flatten(2))  # B x Lq x H\n",
    "\n",
    "        return h, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66048\n",
      "torch.Size([3, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionLayer(128, 4)\n",
    "print(sum([p.numel() for p in mha.parameters()]))\n",
    "q = torch.randn(3, 5, 128)\n",
    "k = torch.randn(3, 7, 128)\n",
    "v = torch.randn(3, 7, 128)\n",
    "\n",
    "h, attn = mha.forward(q, k, v)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Блок энкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/transformer-encoder.png\" width=\"400\"/>\n",
    "<!-- \n",
    "- LayerNorm\n",
    "- SelfAttention\n",
    "- LayerNorn\n",
    "- FeedForward -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, fc_expand: int = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.mha = MultiHeadAttentionLayer(hidden_dim, num_heads)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * fc_expand),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * fc_expand, hidden_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, h: Tensor, mask: Tensor | None = None) -> Tensor:\n",
    "        # self-attention, skip connection, layernorm\n",
    "        ...\n",
    "        # feed-forward, skip connection, layernorm\n",
    "        ...\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132480\n",
      "torch.Size([3, 7, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderBlock(128, 4)\n",
    "print(sum([p.numel() for p in encoder.parameters()]))\n",
    "\n",
    "h = torch.randn(3, 7, 128)\n",
    "\n",
    "h = encoder.forward(h)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданим модуль для энкодера с произвольным количеством блоков.\n",
    "\n",
    "Внутри него будем:\n",
    "- Получать эмбеддинг из токенов и позиций\n",
    "- Обновлять эмбеддинги серией блоков энкодера\n",
    "\n",
    "Пример модели, состоящей только из энкодер-блоков: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        max_length: int = 100,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderBlock(hidden_dim, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        B, T = src.shape\n",
    "        # тензор с номерами позиций\n",
    "        pos = torch.arange(0, T).unsqueeze(0).repeat(B, 1).to(src.device)\n",
    "        \n",
    "        # стартовый эмбеддинг: сумма эмбеддинга токена и эмбеддинга позиции\n",
    "        h = self.token_embedding(src) + self.position_embedding(pos)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, src_mask)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "src = torch.randint(0, 20, size=(3, 5))\n",
    "encoder = Encoder(vocab_size=20, hidden_dim=8, n_heads=2, n_layers=2)\n",
    "encoded = encoder.forward(src, None)\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Блок декодера\n",
    "\n",
    "То же самое, два отличия:\n",
    "- в блоке self-attention можно смотреть только на предыдущие токены\n",
    "- добавлен блок cross-attention для получения информации из декодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/transformer-decoder.png\" width=\"400\"/>\n",
    "\n",
    "<!-- |<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/transformer-decoder.png\" width=\"300\"/> |$E = mc^2$|\n",
    "|-|-| -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, fc_expand: int = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm_3 = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, num_heads)\n",
    "        self.cross_attention = MultiHeadAttentionLayer(hidden_dim, num_heads)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * fc_expand),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * fc_expand, hidden_dim),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        h: Tensor,\n",
    "        tgt_mask: Tensor | None,\n",
    "        input_embeds: Tensor,\n",
    "        src_mask: Tensor | None,\n",
    "    ) -> Tensor:\n",
    "\n",
    "        # masked self-attention, skip connection, layernorm\n",
    "        ...\n",
    "\n",
    "        # cross-attention, skip connection, layernorm\n",
    "        ...\n",
    "\n",
    "        # feed-forward, skip connection, layernorm\n",
    "        ...\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "torch.Size([3, 9, 128])\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderBlock(128, 4)\n",
    "print(sum([p.numel() for p in encoder.parameters()]))\n",
    "\n",
    "tgt = torch.randn(3, 9, 128)\n",
    "\n",
    "tgt = decoder.forward(tgt, None, h, None)\n",
    "print(tgt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданим модуль для декодера с произвольным количеством блоков.\n",
    "\n",
    "Внутри него будем:\n",
    "- Получать эмбеддинг из токенов и позиций (как в энкодере)\n",
    "- Обновлять эмбеддинги серией блоков декодера\n",
    "- Получать логиты над словарём из эмбеддингов\n",
    "\n",
    "Пример модели, состоящей только из декодер-блоков: GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        max_length: int = 100,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    hidden_dim,\n",
    "                    n_heads,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self, tgt: Tensor, encoded_src: Tensor, tgt_mask: Tensor, src_mask: Tensor\n",
    "    ) -> Tensor:\n",
    "        B, T = tgt.shape\n",
    "        # тензор с номерами позиций\n",
    "        pos = torch.arange(0, T).unsqueeze(0).repeat(B, 1).to(tgt.device)\n",
    "\n",
    "        # стартовый эмбеддинг: сумма эмбеддинга токена и эмбеддинга позиции\n",
    "        tgt = self.token_embedding(tgt) + self.position_embedding(pos)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer = cast(DecoderBlock, layer)\n",
    "            tgt = layer.forward(tgt, tgt_mask, encoded_src, src_mask)\n",
    "\n",
    "        # логиты\n",
    "        output = self.fc_out(tgt)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 20])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size=20, hidden_dim=8, n_layers=2, n_heads=2)\n",
    "tgt = torch.randint(0, 20, size=(3, 7))\n",
    "logits = decoder.forward(tgt, encoded, None, None)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Собираем трансформер\n",
    "\n",
    "Примечание: трансформером теперь называют не только encoder-decoder модели (vanilla transformer, T5) но и encoder-only модели (BERT), и decoder-only модели (GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/49787234/103397155-ff354180-4b71-11eb-8283-1c0f50f5b462.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        pad_token_id,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def make_src_mask(self, src: Tensor):\n",
    "        src_mask = (src != self.pad_token_id).unsqueeze(1).unsqueeze(2)  # B x 1 x 1 x T\n",
    "        return src_mask\n",
    "\n",
    "    def make_tgt_mask(self, tgt: Tensor):\n",
    "        B, T = tgt.shape\n",
    "        tgt_pad_mask = (\n",
    "            (tgt != self.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        )  # B x 1 x 1 x T\n",
    "        tgt_sub_mask = torch.tril(torch.ones((T, T), device=tgt.device)).bool()  # T x T\n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask  # B x 1 x T x T\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        # строим маски для энкодера и декодера\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_tgt_mask(tgt)\n",
    "\n",
    "        encoded_src = self.encoder(src, src_mask)\n",
    "        logits = self.decoder(tgt, encoded_src, trg_mask, src_mask)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self, src: Tensor, bos_token_id: int, max_new_tokens: int = 20\n",
    "    ) -> Tensor:\n",
    "        idx = torch.full((src.shape[0], 1), fill_value=bos_token_id)\n",
    "        for t in range(max_new_tokens):\n",
    "            logits = self.forward(src, idx)[:, -1]\n",
    "            new_token = logits.argmax(dim=-1, keepdim=True)\n",
    "            idx = torch.cat([idx, new_token], dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26054757\n",
      "torch.Size([3, 9, 32101])\n"
     ]
    }
   ],
   "source": [
    "src = torch.randint(0, 20, size=(3, 5))\n",
    "tgt = torch.randint(0, 20, size=(3, 9))\n",
    "\n",
    "encoder = Encoder(\n",
    "    vocab_size=len(multi30k.tokenizer), hidden_dim=256, n_layers=1, n_heads=4\n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size=len(multi30k.tokenizer), hidden_dim=256, n_layers=1, n_heads=4\n",
    ")\n",
    "transformer = Transformer(encoder, decoder, pad_token_id=0)\n",
    "\n",
    "print(sum([p.numel() for p in transformer.parameters()]))\n",
    "print(transformer.forward(src, tgt).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Пробуем обучить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "encoder = Encoder(\n",
    "    vocab_size=len(multi30k.tokenizer), hidden_dim=256, n_layers=1, n_heads=4\n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size=len(multi30k.tokenizer), hidden_dim=256, n_layers=1, n_heads=4\n",
    ")\n",
    "transformer = Transformer(encoder, decoder, pad_token_id=0)\n",
    "seq2seq = Seq2Seq(transformer, multi30k.tokenizer, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/sarapulov/miniconda3/envs/dl-course/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | Transformer | 26.1 M\n",
      "--------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.219   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1ca07bc0db4a8c83440f8f4560623b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(accelerator=\"cpu\", max_epochs=6, limit_train_batches=200, logger=False)\n",
    "trainer.fit(model=seq2seq, train_dataloaders=multi30k.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(\n",
    "    batch: dict[str, Tensor], model: Transformer, tokenizer: T5Tokenizer\n",
    "):\n",
    "    source = batch[\"input_ids\"]\n",
    "    target = batch[\"labels\"]\n",
    "\n",
    "    preds = model.generate(source, tokenizer.bos_token_id, max_new_tokens=20)\n",
    "\n",
    "    # decode\n",
    "\n",
    "    source, target, preds = map(\n",
    "        lambda x: tokenizer.batch_decode(x, skip_special_tokens=True),\n",
    "        (source, target, preds),\n",
    "    )\n",
    "\n",
    "    for src, tgt, pred in zip(source, target, preds):\n",
    "        print(f\"Deutsch: {src}\")\n",
    "        print(f\"English: {tgt}\")\n",
    "        print(f\"Translation: {pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deutsch: Ein Typ arbeitet an einem Gebäude.\n",
      "English: A guy works on a building.\n",
      "Translation: A guy working on a building.\n",
      "\n",
      "Deutsch: Drei Leute sitzen in einer Höhle.\n",
      "English: Three people sit in a cave.\n",
      "Translation: Three people are sitting in a field.\n",
      "\n",
      "Deutsch: Leute, die vor einem Gebäude stehen.\n",
      "English: People standing outside of a building.\n",
      "Translation: People are standing outside in a building.\n",
      "\n",
      "Deutsch: Ein Mann schneidet ste von Bäumen.\n",
      "English: A man cutting branches of trees.\n",
      "Translation: A man is cutting a green slide.\n",
      "\n",
      "Deutsch: Ein Kind planscht im Wasser.\n",
      "English: A child is splashing in the water\n",
      "Translation: A child splashes in the water.\n",
      "\n",
      "Deutsch: Eine schöne Frau spielt auf einer Harfe.\n",
      "English: A pretty woman plays a harpsichord.\n",
      "Translation: A lovely woman playing on a wave.\n",
      "\n",
      "Deutsch: Leute sitzen in einem Zug.\n",
      "English: People sit inside a train.\n",
      "Translation: People sitting in a train.\n",
      "\n",
      "Deutsch: Ein kleines Kind kocht mit einer anderen Person.\n",
      "English: A toddler is cooking with another person.\n",
      "Translation: A small child a child carrying cliff.\n",
      "\n",
      "Deutsch: Ein Mann bereitet am Herd Essen zu.\n",
      "English: A man cooking food on the stove.\n",
      "Translation: Man preparing food to the a beach.\n",
      "\n",
      "Deutsch: Ein am Strand geparktes Auto.\n",
      "English: A car parked at the beach.\n",
      "Translation: A beach a beach at night.\n",
      "\n",
      "Deutsch: Zwei Männer in Schwarz in einer Stadt\n",
      "English: Two men wearing black in a city\n",
      "Translation: Two men are city at a city.\n",
      "\n",
      "Deutsch: Ein junges Mädchen schwimmt in einem Pool\n",
      "English: A young girl swimming in a pool\n",
      "Translation: A young girl swimming in a swimming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate_batch(next(iter(multi30k.test_dataloader())), transformer, multi30k.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
