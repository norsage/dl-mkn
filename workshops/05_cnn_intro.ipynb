{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в CNN, классификация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "План:\n",
    "1. Иллюстрация применения свёртки к изображению\n",
    "2. `nn.Conv2D` и параметры\n",
    "3. Max pooling, Average pooling, `nn.MaxPool2D`\n",
    "4. Пример: классификация на CIFAR10\n",
    "5. Модели в `torchvision`, transfer learning\n",
    "6. Аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lightning==2.0.9 tensorboard==2.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterator\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.utilities.types import (\n",
    "    EVAL_DATALOADERS,\n",
    "    STEP_OUTPUT,\n",
    "    TRAIN_DATALOADERS,\n",
    "    OptimizerLRScheduler,\n",
    ")\n",
    "from PIL import Image\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Свёртка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d2l.ai/_images/conv-pad.svg\" style=\"background:white\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2)\n",
    "for k, v in conv.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Grosser_Panda.JPG/440px-Grosser_Panda.JPG\"\n",
    "\n",
    "img = Image.open(requests.get(IMAGE_URL, stream=True).raw)\n",
    "grayscale = img.convert(\"L\")\n",
    "\n",
    "plt.imshow(grayscale, cmap=\"gray\")\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим несколько фильтров. Что они делают?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\n",
    "    \"0\": torch.tensor([[[\n",
    "        [ 0,  0,  0],\n",
    "        [ 0,  1,  0],\n",
    "        [ 0,  0,  0],\n",
    "    ]]], dtype=torch.float),\n",
    "    \"1\": torch.tensor([[[\n",
    "        [ 0, -1,  0],\n",
    "        [-1,  4, -1],\n",
    "        [ 0, -1,  0],\n",
    "    ]]], dtype=torch.float),\n",
    "    \"2\": torch.tensor([[[\n",
    "        [ 0, -1,  0],\n",
    "        [-1,  5, -1],\n",
    "        [ 0, -1,  0],\n",
    "    ]]], dtype=torch.float),\n",
    "    \"3\": torch.tensor([[[\n",
    "        [ 1,  2,  1],\n",
    "        [ 2,  4,  2],\n",
    "        [ 1,  2,  1],\n",
    "    ]]], dtype=torch.float) / 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(10, 7))\n",
    "\n",
    "# Создадим свёрточный слой и преобразуем изображение в тензор\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
    "img_tensor = transforms.ToTensor()(grayscale)\n",
    "\n",
    "for k, (filter_name, filter_kernel) in enumerate(filters.items()):\n",
    "    # подставим значения для фильтра в свёрточный слой\n",
    "    conv.weight.data = filter_kernel\n",
    "\n",
    "    # преобразуем входное изображение и обрежем до отрезка [0, 1] для корректного отображения\n",
    "    transformed = conv(img_tensor).detach().numpy()[0].clip(0, 1)\n",
    "    row = k // ncols\n",
    "    column = k % ncols\n",
    "    ax = axes[row][column]\n",
    "    ax.imshow(transformed, cmap=\"gray\")\n",
    "    ax.set_title(filter_name)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `nn.Conv2D`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свёртка входного тензора размера $(N, C_{\\text{in}}, H, W)$ в тензор размера $(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ описывается так:\n",
    "\n",
    "<!-- In the simplest case, the output value of the layer with input size\n",
    "$(N, C_{\\text{in}}, H, W)$ and output $(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$    can be precisely described as: -->\n",
    "    \n",
    "$$\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) + \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)$$\n",
    "\n",
    "$\\star$ - 2D оператор кросс-корреляции, $N$ - размер батча, $C$ - количество каналов, $H$ и $W$ - высота и ширина в пикселах соответственно\n",
    "\n",
    "<!-- $\\star$ is the valid 2D `cross-correlation`_ operator,$N$ is a batch size, $C$ denotes a number of channels, $H$ is a height of input planes in pixels, and $W$ is width in pixels. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый `nn.Conv2D` слой содержит две группы параметров:\n",
    "- `weight`: тензор размера $(C_{out}, C_{in}, H_k, W_k)$\n",
    "  - $C_{in}$: количество входных каналов\n",
    "  - $C_{out}$: количество выходных каналов\n",
    "  - $(H_k, W_k)$: размер 2D кернела (например, $3 \\times 3$)\n",
    "- `bias`: тензор размера $(C_{out},)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, регулировать количество параметров можно с помощью аргументов конструктора\n",
    "- `bias (bool)`: включение / отключения параметра сдвига\n",
    "- `groups`: число групп, на которые следует разбить входные и выходные каналы; каждый фильтр работает только с одной группой каналов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GroupConv](https://www.researchgate.net/publication/351823568/figure/fig2/AS:1027082138181632@1621886872538/Standard-convolution-left-and-group-convolution-right.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**: сколько параметров будет в свёрточном слое с размером кернела $3 \\times 3$ для преобразования тензора с 3 каналами в тензор с 6 каналами без группировки? С разделением на 3 группы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие параметры, влияющие на результат свёртки\n",
    "- `stride`: каков размер смещения между соседними срезами, к которым применяется свёртка\n",
    "- `padding`: как дополнить входной тензор пикселами вокруг\n",
    "- `dilation`: позволяет взять не связный срез, а растянутый, с просветом между пикселами\n",
    "\n",
    "Демо: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides.gif?raw=true\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/arbitrary_padding_no_strides.gif?raw=true\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/same_padding_no_strides.gif?raw=true\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/full_padding_no_strides.gif?raw=true\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides</td>\n",
    "    <td>Arbitrary padding, no strides</td>\n",
    "    <td>Half padding, no strides</td>\n",
    "    <td>Full padding, no strides</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_strides.gif?raw=true\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides.gif?raw=true\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides_odd.gif?raw=true\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/dilation.gif?raw=true\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides</td>\n",
    "    <td>Padding, strides</td>\n",
    "    <td>Padding, strides (odd)</td>\n",
    "    <td>No padding, no stride, dilation</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![maxpool](https://production-media.paperswithcode.com/methods/MaxpoolSample2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры инициализации `nn.MaxPool2d` (а так же `nn.AvgPool2d`) похожи на свёрточный слой:\n",
    "- `kernel_size`: размер 2D кернела (обычно $2 \\times 2$)\n",
    "- `stride`: каков размер смещения между соседними срезами, к которым применяется pooling (обычно 2, т.е. без наложения)\n",
    "- `padding`: как дополнить входной тензор пикселами вокруг\n",
    "- `dilation`: позволяет взять не связный срез, а растянутый, с просветом между пикселами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Характерный строительный блок для свёрточной сети:\n",
    "- `Conv -> ReLU -> MaxPool`\n",
    "- В каждом следующем блоке размер активаций уменьшается (обычно в 4 раза), размер каналов увеличивается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы перед линейным слоем размер активаций был одинаковым для любого размера входного изображения, есть `nn.AdaptiveMaxPool2d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Пример: классификация CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модуль данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        transform: Callable[[Image.Image], Tensor],\n",
    "        num_workers: int = 0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = datasets.CIFAR10(\n",
    "                root=\"./data\", train=True, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "        if stage in (\"fit\", \"validate\"):\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                root=\"./data\", train=False, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Stage '{stage}' not implemented for CIFAR10\")\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "datamodule = CIFAR10(batch_size=32, transform=transform, num_workers=0)\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на примеры изображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 5\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(10, 3))\n",
    "for k, label in enumerate(datamodule.train_dataset.classes):\n",
    "    example = next(img for img, target in datamodule.train_dataset if target == k)\n",
    "    # unnormalize images\n",
    "    example = 0.5 * example + 0.5\n",
    "    row = k // ncols\n",
    "    column = k % ncols\n",
    "    ax = axes[row][column]\n",
    "    ax.imshow(transforms.ToPILImage()(example), cmap=\"gray\")\n",
    "    ax.set_title(label)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightningModule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVisionModel(nn.Module):\n",
    "    def forward(self, images: Tensor) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        ...\n",
    "\n",
    "\n",
    "class VisionClassifier(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BaseVisionModel,\n",
    "        optimizer_fn: Callable[[Iterator[nn.Parameter]], Optimizer],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим простую архитектуру: 2 блока из свёртки и maxpooling, три линейных слоя, ReLU в качестве нелинейности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для запуска обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    model: BaseVisionModel,\n",
    "    datamodule: L.LightningDataModule,\n",
    "    **trainer_kwargs,\n",
    ") -> None:\n",
    "    # создадим LightningModule\n",
    "    ...\n",
    "    # создадим логгер\n",
    "    logger = TensorBoardLogger(save_dir=\"logs\", name=\"cifar-10\")\n",
    "\n",
    "    # создадим Trainer и запустим обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard локально: `tensorboard --logdir logs/cifar-10`\n",
    "\n",
    "Если запускаем в Colab - активируем расширение `tensorboard` прямо в ноутбуке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir logs/cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(\n",
    "    model=Net(num_classes=10),\n",
    "    datamodule=datamodule,\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. `torchvision.models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание модели с нужными параметрами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import MobileNetV2\n",
    "\n",
    "mobilenet = MobileNetV2(\n",
    "    num_classes=10,\n",
    "    width_mult=1.0,\n",
    "    inverted_residual_setting=[\n",
    "        # t, c, n, s\n",
    "        [1, 16, 1, 1],\n",
    "        [3, 24, 2, 2],\n",
    "        [3, 32, 3, 2],\n",
    "    ],\n",
    "    dropout=0.2,\n",
    ")\n",
    "# наш LightningModule ожидает, что у модели есть поле num_classes, чтобы правильно инициализировать метрики\n",
    "mobilenet.num_classes = 10\n",
    "\n",
    "run_training(\n",
    "    model=mobilenet,\n",
    "    datamodule=datamodule,\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование модели, предобученной на ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.efficientnet import EfficientNet_B0_Weights, efficientnet_b0\n",
    "\n",
    "# создаём EfficientNet с весами, полученными на ImageNet\n",
    "weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "efficientnet = efficientnet_b0(weights=weights)\n",
    "\n",
    "# замораживаем веса\n",
    "for param in efficientnet.features[:-1].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# меняем последний линейный слой\n",
    "efficientnet.classifier[-1] = nn.Linear(\n",
    "    in_features=efficientnet.classifier[-1].weight.shape[1], out_features=10\n",
    ")\n",
    "efficientnet.num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобученные модели на ImageNet ожидают специальным образом трансформированные изображения. Здесь нам и пригодится параметр `transform` в нашем `LightningDataModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CIFAR10(\n",
    "    batch_size=32,\n",
    "    transform=weights.transforms(),\n",
    "    num_workers=0,\n",
    ")\n",
    "run_training(model=efficientnet, datamodule=datamodule, accelerator=\"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Аугментации, `torchvision.transforms` и `albumentations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`albumentations`\n",
    "\n",
    "![albumentations](https://albumentations.ai/assets/img/custom/top_image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите модель на CIFAR-10, дающую точность более 90% на тестовой выборке. Постарайтесь уложиться модель с ~5 млн параметров\n",
    "\n",
    "Можно использовать:\n",
    "- предобученные модели на ImageNet\n",
    "- аугментации из `torchvision.transforms` и `albumentations`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
