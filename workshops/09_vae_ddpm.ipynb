{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## От VAE к DDPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "План на сегодня: реализуем генерацию лиц с обучением\n",
    "- Variational autoencoder (VAE)\n",
    "- Denoising diffusion probalilistic model (DDPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовим данные\n",
    "\n",
    "Всё с прошлой недели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, cast\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from PIL import Image\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datadir: Path,\n",
    "        transform: Callable[[Image.Image], Tensor],\n",
    "        pattern: str = \"*.jpg\",\n",
    "    ) -> None:\n",
    "        self.images = list(datadir.rglob(pattern))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index) -> Tensor:\n",
    "        img = Image.open(self.images[index])\n",
    "        return self.transform(img)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если работаете локально или в Google Colab, укажите правильный путь до папки с изображениями\n",
    "datadir = Path(\"/kaggle/input/celeba-dataset/img_align_celeba\")\n",
    "dataset = ImageDataset(\n",
    "    datadir,\n",
    "    transform=transform,\n",
    ")\n",
    "print(len(dataset))\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grid = torchvision.utils.make_grid(\n",
    "    tensor=batch,\n",
    "    nrow=8,\n",
    "    padding=2,\n",
    "    normalize=False,\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    torchvision.transforms.ToPILImage()(\n",
    "        img_grid * torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)\n",
    "        + torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличный блогпост со всей математикой: https://lilianweng.github.io/posts/2018-08-12-vae/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычный автоэнкодер:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/autoencoder-architecture.png\" style=\"background:white\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариационный автоэнкодер:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/vae-gaussian.png\" style=\"background:white\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/reparameterization-trick.png\" style=\"background:white\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Описание обучения и семплинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим максимизировать evidence lower bound (ELBO):\n",
    "\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\mathcal{L}(\\theta, \\phi; x)\n",
    "&= \\log p_\\theta(\\mathbf{x}) - D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}\\vert\\mathbf{x}) )\\\\\n",
    "&= \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})} \\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) - D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}) ) \\\\\n",
    "\\theta^{*}, \\phi^{*} &= \\arg\\max_{\\theta, \\phi} \\mathcal{L}(\\theta, \\phi; x)\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка для обучения VAE:\n",
    "\n",
    "$L_\\text{VAE}(\\theta, \\phi)  = - \\mathcal{L}(\\theta, \\phi; x) = - \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})} \\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) + D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}) ) \\\\$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае нормального распределения:\n",
    "\n",
    "$$L_\\text{VAE}(\\theta, \\phi) = \\frac{1}{2c} \\mathbb{E}[\\lVert x - f(z) \\rVert^2] + \\frac{1}{2}\\sum_{i = 1}^d (\\sigma_{x, j}^2 + \\mu_{x, j}^2 - \\log \\sigma_{x, j}^2 - 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем пользоваться ошибкой для $\\beta$-VAE ([Higgins et al., 2017](https://openreview.net/forum?id=Sy2fzU9gl)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_\\text{BETA}(\\phi, \\beta) = - \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})} \\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) + \\beta D_\\text{KL}(q_\\phi(\\mathbf{z}\\vert\\mathbf{x})\\|p_\\theta(\\mathbf{z}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.types import OptimizerLRScheduler\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    latent_size: int\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        ...\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    latent_size: int\n",
    "\n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class VAE(L.LightningModule):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, beta: float = 0.0005):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "\n",
    "    def training_step(self, batch: Tensor, batch_idx: int) -> None:\n",
    "        # encode + decode\n",
    "        mu, logvar = self.encoder(batch)\n",
    "        eps = torch.randn_like(mu)\n",
    "        z = mu + eps * torch.exp(0.5 * logvar)\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        # calculate loss\n",
    "        reconstruction_loss = F.mse_loss(batch, x_hat)\n",
    "        kl = (torch.exp(logvar) + mu**2 - logvar - 1).sum(dim=1).mean()\n",
    "        loss = reconstruction_loss + self.beta * kl\n",
    "        self.log(\"reconstruction_loss\", reconstruction_loss, prog_bar=True)\n",
    "        self.log(\"kl\", kl, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, n: int) -> Tensor:\n",
    "        device = next(self.decoder.parameters()).device\n",
    "        z = torch.randn(n, self.decoder.latent_size, 1, 1, device=device)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(), lr=0.0001, betas=(0.5, 0.999)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Энкодер и декодер\n",
    "\n",
    "Тоже для сравнимости возьмём что-то с прошлой недели.\n",
    "\n",
    "Наш энкодер похож на дискриминатор для GAN, но возвращает параметры для нормального распределения: матожидание и логарифм дисперсии\n",
    "\n",
    "А декодер - архитектурно один в один с генератором для GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(latent_size, 64 * 8, 4, 1, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64 * 8)\n",
    "        self.conv2 = nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64 * 4)\n",
    "        self.conv3 = nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64 * 2)\n",
    "        self.conv4 = nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv5 = nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        x = z.view(-1, self.latent_size, 1, 1)  # B x C x H x W\n",
    "        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        x = F.relu(self.bn2(self.conv2(x)), inplace=True)\n",
    "        x = F.relu(self.bn3(self.conv3(x)), inplace=True)\n",
    "        x = F.relu(self.bn4(self.conv4(x)), inplace=True)\n",
    "        return torch.tanh(self.conv5(x))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64 * 2)\n",
    "        self.conv3 = nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64 * 4)\n",
    "        self.conv4 = nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64 * 8)\n",
    "        self.conv5 = nn.Conv2d(64 * 8, 2 * latent_size, 4, 1, 0, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x) -> tuple[Tensor, Tensor]:\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.bn1(self.conv2(x)), negative_slope=0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.bn2(self.conv3(x)), negative_slope=0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.bn3(self.conv4(x)), negative_slope=0.2, inplace=True)\n",
    "        x = self.conv5(x).flatten(1)\n",
    "        mu = x[:, :self.latent_size]\n",
    "        logvar = x[:, self.latent_size:]\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 128\n",
    "decoder = Decoder(latent_size)\n",
    "encoder = Encoder(latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что всё работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = encoder.forward(batch)\n",
    "epsilon = torch.randn_like(mu, requires_grad=True)\n",
    "z = mu + epsilon * sigma\n",
    "print(mu.shape)\n",
    "g = decoder.forward(z)\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Запуск обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам понадобится callback, который будет выбирать несколько изображений из датасета, реконструировать их, и генерировать несколько новых из шума. Это позволит нам визуально отслеживать степень регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate(L.Callback):\n",
    "    def __init__(self, n: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: L.Trainer, pl_module: L.LightningModule) -> None:\n",
    "        # original\n",
    "        loader = DataLoader(trainer.train_dataloader.dataset, batch_size=self.n, shuffle=True)\n",
    "        x = next(iter(loader)).to(device=pl_module.device)\n",
    "\n",
    "        # reconstructed\n",
    "        mu, logvar = pl_module.encoder(x)\n",
    "        std = torch.exp(logvar / 2)\n",
    "        eps = torch.randn_like(mu)\n",
    "        z = eps * std + mu\n",
    "        x_hat = pl_module.decoder(z)\n",
    "\n",
    "        # new\n",
    "        z = torch.randn_like(mu)\n",
    "        x_new = pl_module.decoder(z)\n",
    "\n",
    "\n",
    "        images = torch.cat([x, x_hat, x_new])\n",
    "        grid = torchvision.utils.make_grid(\n",
    "            tensor=images,\n",
    "            nrow=min(self.n, 8),\n",
    "            padding=2,\n",
    "            normalize=True,\n",
    "        )\n",
    "        str_title = f\"{pl_module.__class__.__name__}_images\"\n",
    "        logger = cast(TensorBoardLogger, pl_module.logger)\n",
    "        logger.experiment.add_image(str_title, grid, global_step=pl_module.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=200,\n",
    "    logger=TensorBoardLogger(save_dir=\"logs\", name=\"vae\"),\n",
    "    limit_train_batches=25,\n",
    "    callbacks=[Generate(n=8)],\n",
    ")\n",
    "\n",
    "latent_size = 128\n",
    "lit = VAE(encoder=Encoder(latent_size), decoder=Decoder(latent_size), beta=0.0005)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=lit, train_dataloaders=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"version_0\"\n",
    "checkpoint_path = next(Path(f\"logs/vae/{model_version}/checkpoints/\").glob(\"*.ckpt\"))\n",
    "lit.load_state_dict(torch.load(checkpoint_path)[\"state_dict\"])\n",
    "images = lit.sample(n=64)\n",
    "grid = torchvision.utils.make_grid(\n",
    "    tensor=images,\n",
    "    nrow=8,\n",
    "    padding=2,\n",
    "    normalize=True,\n",
    "    # range=(-1, 1),\n",
    ")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.imshow(torchvision.transforms.ToPILImage()(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Упражнения\n",
    "\n",
    "1. Сделайте интерполяцию между двумя латентными векторами и сгенерируйте изображения лиц для полученной последовательности. Вам пригодится функция `torch.lerp`. Почитайте про сферическую линейную интерполяцию (SLERP), сравните результаты с простой линейной.\n",
    "2. Реализуйте VQ-VAE (https://paperswithcode.com/method/vq-vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Denoising diffusion probabilistic model (DDPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png\" style=\"background:white\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/DDPM.png\" style=\"background:white\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Прямая диффузия** с расписанием $\\{ \\beta_t \\in (0, 1) \\}_{t = 1}^T$:\n",
    "\n",
    "$q(x_t | x_{t-1}) = \\mathcal{N}(x_t ; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t \\mathbf{I})$\n",
    "\n",
    "$q(x_{1:T} | x_0) = \\prod_{t = 1}^T q(x_t | x_{t - 1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трюк с репараметризацией позволяет получить в явном виде выражение для $x_t$ от $x_0$:\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\alpha_t\n",
    "&= 1 - \\beta_t \\\\\n",
    "\\bar{\\alpha}_t &= \\prod_{i=1}^t \\alpha_i\n",
    "\\end{aligned}$\n",
    "\n",
    "$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$\n",
    "\n",
    "<!-- \\mathcal{L}(\\theta, \\phi; x)\n",
    "&= \\log p_\\theta(\\mathbf{x}) - D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}\\vert\\mathbf{x}) )\\\\\n",
    "&= \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})} \\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) - D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}) ) \\\\\n",
    "\\theta^{*}, \\phi^{*} &= \\arg\\max_{\\theta, \\phi} \\mathcal{L}(\\theta, \\phi; x) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обратная диффузия**\n",
    "\n",
    "$p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t, t))$ - похоже на VAE?\n",
    "\n",
    "$\\mu_{\\theta}(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta}(x_t, t) \\right)$\n",
    "\n",
    "В качестве $\\sigma_t^2$ берётся $\\beta_t$ или $\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t$, но можно пытаться выучивать и её"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/DDPM-algo.png\" style=\"background:white\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Описание обучения и семплинга\n",
    "\n",
    "Перенесём уравнения из Algorithm 1 в `training_step`, из Algorithm 2 - в `sample`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "    # описание интерфейса модели\n",
    "    def forward(self, x: Tensor, t: Tensor) -> Tensor:\n",
    "        # x - батч с зашумлённым изображениями (B x 3 x 64 x 64)\n",
    "        # t - номер шага (B x 1)\n",
    "        ...\n",
    "\n",
    "\n",
    "class DDPM(L.LightningModule):\n",
    "    alpha: Tensor\n",
    "    alpha_bar: Tensor\n",
    "    beta: Tensor\n",
    "    sigma: Tensor\n",
    "    T: int\n",
    "\n",
    "    def __init__(self, model: Denoiser, beta_0: float, beta_T: float, T: int):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.T = T\n",
    "\n",
    "        # Compute diffusion hyperparameters\n",
    "        beta = torch.linspace(beta_0, beta_T, T)\n",
    "        alpha = 1. - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "        sigma = torch.sqrt(beta)\n",
    "        sigma[1:] *= torch.sqrt((1-alpha_bar[:-1]) / (1-alpha_bar[1:]))\n",
    "\n",
    "        self.register_buffer(\"alpha\", alpha)\n",
    "        self.register_buffer(\"alpha_bar\", alpha_bar)\n",
    "        self.register_buffer(\"beta\", beta)\n",
    "        self.register_buffer(\"sigma\", sigma)\n",
    "\n",
    "    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        x_0 = batch\n",
    "        t = torch.randint(0, self.T, size=(batch.shape[0], 1, 1, 1), device=batch.device)\n",
    "        eps = torch.randn_like(x_0)\n",
    "\n",
    "        # forward diffusion\n",
    "        x_t = torch.sqrt(self.alpha_bar[t]) * x_0 + torch.sqrt(1 - self.alpha_bar[t]) * eps\n",
    "\n",
    "        # predict added noise and get MSE error\n",
    "        eps_theta = self.model.forward(x_t, t.view(-1, 1))\n",
    "        loss = F.mse_loss(eps_theta, eps)\n",
    "        self.log(\"reconstr_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def sample(self, n: int) -> Tensor:\n",
    "        device = next(self.parameters()).device\n",
    "        x_t = torch.randn(n, 3, 64, 64, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t in range(self.T - 1, -1, -1):\n",
    "                z = torch.randn_like(x_t) if t > 0 else 0.\n",
    "                ts = torch.full((n, 1), fill_value=t, device=device)\n",
    "                # predict epsilon and calculate expectation\n",
    "                eps_theta = self.model.forward(x_t, ts)\n",
    "                mu_theta = 1 / torch.sqrt(self.alpha[t]) * (x_t - (1 - self.alpha[t]) / (torch.sqrt(1 - self.alpha_bar[t])) * eps_theta)\n",
    "                \n",
    "                # get new x_t\n",
    "                x_t = mu_theta + self.sigma[t] * z\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        optim = torch.optim.AdamW(\n",
    "            self.parameters(), lr=0.0002,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optim,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Архитектура модели: time-conditioned U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера возьмём U-Net из практики по сегментации, но его придётся модифицировать: теперь на вход модели приходит не только изображение, но и номер шага\n",
    "\n",
    "Кодировать номер шага можно, напрмер, используя синусоидальные эмбеддинги, как в оригинальном трансформере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d - желаемый размер вектора, кодирующего позицию\n",
    "for \n",
    "$\n",
    "i \\in [1, d] \\\\\n",
    "\\begin{aligned}\n",
    "PE(t, 2i) = \\sin \\left( \\frac{t}{10000^{2i / d}} \\right) \\\\\n",
    "PE(t, 2i + 1) = \\cos \\left( \\frac{t}{10000^{2i / d}} \\right)\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    pe: Tensor\n",
    "\n",
    "    def __init__(self, encoding_size: int, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, encoding_size, 2) * (-math.log(10000.0) / encoding_size))\n",
    "        pe = torch.zeros(max_len, encoding_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, t: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            t: Tensor, shape ``[batch_size]``\n",
    "        \"\"\"\n",
    "        return self.pe[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 4))\n",
    "\n",
    "plt.imshow(PositionalEncoding(128)(torch.arange(1000)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внесём правки в модули U-Net, чтобы мы могли передать номер шага в свёрточные слои. Как это сделать?\n",
    "\n",
    "Подсказка: `PositionalEncoding` можно добавить внутрь U-Net для кодирования номера шага, чтобы внутренние блоки получали уже векторное представление шага и как-то добавляли его к каналам изображения. Можно попробовать\n",
    "- конкатенацию: C' = C + encoding_size, то есть каналов станет больше\n",
    "- сложение: линейно преобразуем закодированный номер шага к нужному размеру с помощью линейного слоя (`nn.Linear`: $\\mathbb{R}^d \\rightarrow \\mathbb{R}^C$) и прибавляем вдоль размерности каналов; broadcasting в помощь!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, mid_channels: int | None = None, t_size: int = 128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.linear = nn.Linear(t_size, in_channels)\n",
    "\n",
    "    def forward(self, x, t: Tensor | None = None):\n",
    "        # x:             B x C x H x W\n",
    "        # t: B x 128  -> B x C x 1 x 1\n",
    "        # x + t\n",
    "        B, C, _, _ = x.shape\n",
    "        if t is not None:\n",
    "            t = self.linear.forward(t).view(B, -1, 1, 1) # B x C x 1 x 1\n",
    "            x = x + t\n",
    "        \n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, t_size: int = 128):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels, t_size=t_size)\n",
    "        # self.maxpool_conv = nn.Sequential(\n",
    "        #     nn.MaxPool2d(2), DoubleConv(in_channels, out_channels, t_size=t_size)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.pool(x)\n",
    "        return self.conv.forward(x, t)\n",
    "        # return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, bilinear: bool = True, t_size: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2, t_size=t_size)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels, t_size=t_size)\n",
    "\n",
    "    def forward(self, x1, x2, t):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels: int,\n",
    "        n_classes: int,\n",
    "        channel_sizes: list[int],\n",
    "        bilinear: bool = False,\n",
    "        t_size: int = 32,  # размер PositionEncoding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_conv = DoubleConv(in_channels=n_channels, out_channels=channel_sizes[0])\n",
    "        self.downscaler = self.create_downscaler(channel_sizes, bilinear, t_size)\n",
    "        self.upscaler = self.create_upscaler(channel_sizes[::-1], bilinear, t_size)\n",
    "        self.head = nn.Conv2d(channel_sizes[0], n_classes, kernel_size=1)\n",
    "        self.pos_encoder = PositionalEncoding(t_size)\n",
    "\n",
    "    @classmethod\n",
    "    def create_downscaler(\n",
    "        cls, channel_sizes: list[int], bilinear: bool = False, t_size: int = 128\n",
    "    ) -> nn.ModuleList:\n",
    "        factor = 2 if bilinear else 1\n",
    "        down_factors = [1] * (len(channel_sizes) - 2) + [factor]\n",
    "        return nn.ModuleList(\n",
    "            [\n",
    "                Down(fin, fout // dfactor, t_size)\n",
    "                for fin, fout, dfactor in zip(\n",
    "                    channel_sizes[:-1], channel_sizes[1:], down_factors\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def create_upscaler(\n",
    "        cls, channel_sizes: list[int], bilinear: bool = False, t_size: int = 128\n",
    "    ) -> nn.ModuleList:\n",
    "        factor = 2 if bilinear else 1\n",
    "        up_factors = [factor] * (len(channel_sizes) - 2) + [1]\n",
    "        return nn.ModuleList(\n",
    "            [\n",
    "                Up(fin, fout // ufactor, bilinear=bilinear, t_size=t_size)\n",
    "                for fin, fout, ufactor in list(\n",
    "                    zip(channel_sizes[:-1], channel_sizes[1:], up_factors)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor) -> Tensor:\n",
    "        # t: B x 1 -> B x t_size\n",
    "        t = self.pos_encoder.forward(t)  # B x t_size\n",
    "        xs = []\n",
    "        x = self.in_conv(x)\n",
    "        for conv in self.downscaler:\n",
    "            conv = cast(Down, conv)\n",
    "            xs.append(x)\n",
    "            x = conv.forward(x, t)\n",
    "\n",
    "        xs.reverse()\n",
    "        for conv, act in zip(self.upscaler, xs):\n",
    "            conv = cast(Up, conv)\n",
    "            x = conv.forward(x, act, t)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=3, n_classes=3, channel_sizes=[64, 128, 256, 512], bilinear=True)\n",
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = torch.randn(16, 3, 64, 64)\n",
    "t = torch.randint(0, 1000, size=(16, 1))\n",
    "eps = model.forward(x_0, t)\n",
    "print(eps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Запуск обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем похожий callback для отрисовки примеров изображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate(L.Callback):\n",
    "    def __init__(self, n: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: L.Trainer, pl_module: L.LightningModule) -> None:\n",
    "        images = pl_module.sample(self.n)\n",
    "        grid = torchvision.utils.make_grid(\n",
    "            tensor=images,\n",
    "            nrow=8,\n",
    "            padding=2,\n",
    "            normalize=True,\n",
    "        )\n",
    "        str_title = f\"{pl_module.__class__.__name__}_images\"\n",
    "        logger = cast(TensorBoardLogger, pl_module.logger)\n",
    "        logger.experiment.add_image(str_title, grid, global_step=pl_module.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=500,\n",
    "    logger=TensorBoardLogger(save_dir=\"logs\", name=\"ddpm\"),\n",
    "    limit_train_batches=100,\n",
    "    callbacks=[Generate(n=16)],\n",
    ")\n",
    "model = UNet(n_channels=3, n_classes=3, channel_sizes=[64, 128, 256, 512], bilinear=False, t_size=32)\n",
    "lit = DDPM(model=model, beta_0=0.0001, beta_T=0.02, T=200)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=lit, train_dataloaders=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"version_0\"\n",
    "checkpoint_path = next(Path(f\"logs/ddpm/{model_version}/checkpoints/\").glob(\"*.ckpt\"))\n",
    "lit.load_state_dict(torch.load(checkpoint_path)[\"state_dict\"])\n",
    "lit.to(device=\"mps\")\n",
    "images = lit.sample(n=64)\n",
    "grid = torchvision.utils.make_grid(\n",
    "    tensor=images,\n",
    "    nrow=8,\n",
    "    padding=2,\n",
    "    normalize=True,\n",
    ")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.imshow(torchvision.transforms.ToPILImage()(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Упражнения\n",
    "\n",
    "Попробуйте добиться хорошего качества генерации лиц.\n",
    "1. Модифицируйте архитектуру. Попробуйте как-то иначе включать в UNet данные о номере шага\n",
    "2. Посмотрите на зависимость качества модели от числа шагов диффузии и от [параметризации расписания](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#parameterization-of-beta_t)\n",
    "3. Добавьте в `configure_optimizers()` управление шагом оптимизатора через `torch.optim.lr_scheduler.CosineAnnealingLR`\n",
    "4. Попробуйте сделать генерацию в несколько шагов: пусть модель сначала генерирует лицо в меньшем разрешении (скажем, 32x32), а затем использует результат в качестве условия для генерации лица в более высоком разрешении."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
