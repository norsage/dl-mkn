{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основы глубокого обучения в NLP. CharRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "План на сегодня: RNN - генератор имён\n",
    "\n",
    "1. Базовая работа с текстом: токенизация, кодирование и декодирование\n",
    "2. Рекуррентные архитектуры: RNN, LSTM, GRU\n",
    "3. Обучение генерации == next token prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Готовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://download.pytorch.org/tutorial/data.zip\n",
    "# ! unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 5 data/names/Chinese.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Пишем датасет и компоновщик батчей\n",
    "\n",
    "Нам нужно:\n",
    "1. Прочитать все имена из текстовых файлов\n",
    "2. Закодировать каждое имя как последовательность целых чисел, предварительно добавив к именам символы начала и окончания (зачем?)\n",
    "3. Сохранить пары (список токенов, id языка)\n",
    "4. Седать разбиение на train/test\n",
    "5. Реализовать сборку примеров в батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesDataset(Dataset):\n",
    "    # псевдоним для пары имя-язык\n",
    "    _ItemPair = tuple[str, int]\n",
    "\n",
    "    vocabulary: dict[str, int]\n",
    "    languages: dict[str, int]\n",
    "    names: list[_ItemPair]\n",
    "\n",
    "    def __init__(self, datadir: Path) -> None:\n",
    "        pad_token = ''\n",
    "        bos_token = '?'  # beginning of sequence\n",
    "        eos_token = '\\n'  # end of sequence\n",
    "        self.vocabulary = {pad_token: 0, bos_token: 1, eos_token: 2}\n",
    "        self.languages = {}\n",
    "        self.names = []\n",
    "        # iterate over files, update vocabulary, save name + language pairs\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocabulary)\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        return len(self.languages)\n",
    "\n",
    "    def encode(self, name: str) -> list[int]:\n",
    "        ...\n",
    "    \n",
    "    def decode(self, encoded: list[int]) -> str:\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[list[int], int]:\n",
    "        return self.names[index]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NamesDataset(Path(\"data/names/\"))\n",
    "tokens, label = dataset[4444]\n",
    "print(tokens, label)\n",
    "print(dataset.decode(tokens), dataset.languages[label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбивка датасета на трейн и тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train_test_split(dataset: NamesDataset, ratio: float = 0.1) -> tuple[NamesDataset, NamesDataset]:\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(dataset, ratio=0.1)\n",
    "print(\"Train size: \", len(train_dataset))\n",
    "print(\"Test size: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упаковка в батчи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: list[tuple[list[int], int]]) -> tuple[Tensor, Tensor]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_dataset[i] for i in range(8)]\n",
    "tokens, labels = collate_fn(batch)\n",
    "print(\"Tokens shape: \", tokens.shape, \"\\nLabels shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем в загрузчик данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "tokens, labels = next(iter(train_loader))\n",
    "print(\"Tokens shape: \", tokens.shape, \"\\nLabels shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Пишем простую RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с написания RNNCell - одного рекуррентного блока\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/02KvP.png\" style=\"background:white\" width=\"600\"/>\n",
    "\n",
    "<!-- <img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" style=\"background:white\" width=\"600\"/> -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    (x_{t}, h_{t-1}) -> h_{t}\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_dim = 10\n",
    "hidden_dim = 8\n",
    "cell = RNNCell(input_dim, hidden_dim)\n",
    "h = torch.randn(1, hidden_dim)\n",
    "# расширяем до размеров батча\n",
    "h_expanded = h.expand((batch_size, -1))\n",
    "x = torch.randn(batch_size, input_dim)\n",
    "h_new = cell.forward(x, h_expanded)\n",
    "print(h_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение 1**: реализуйте более сложно устроенную LSTMCell, где теперь есть:\n",
    "1. два внутренних состояния: cell state $c_t$ и hidden state $h_t$\n",
    "2. набор гейтов для управления обновлениями состояний\n",
    "\n",
    "[blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "<!-- <img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" style=\"background:white\" width=\"600\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" style=\"background:white\" width=\"500\"/>\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" style=\"background:white\" width=\"500\"/>\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" style=\"background:white\" width=\"500\"/>\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" style=\"background:white\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение 2**: реализуйте GRUCell\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\" style=\"background:white\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем модель, состоящую из следующих блоков:\n",
    "1. `embed`: кодирует входные токены в векторы размера `hidden_dim`\n",
    "2. `rnn`: наша рекуррентная ячейка\n",
    "3. `output`: восстанавливает логиты из скрытого состояния `h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение 3**. Добавьте в класс `RNN` возможность\n",
    "   1. Нескольких последовательных рекуррентных слоёв\n",
    "   2. Выбора другого типа рекуррентной ячейки (`GRU`, `LSTM`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "model = RNN(\n",
    "    vocab_size=train_dataset.vocab_size,\n",
    "    hidden_dim=hidden_dim,\n",
    ")\n",
    "model.forward(tokens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Функция для генерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема:\n",
    "1. Подаём на вход произвольный префикс имени, можно только токен начала\n",
    "2. Проходимся моделью по префиксу, получаем логиты для следующего токена\n",
    "3. Семплируем новый токен, добавляем его к префиксу, возвращаемся к шагу 1.\n",
    "4. Критерии остановки:\n",
    "   - встретили символ окончания строки\n",
    "   - сгенерировали максимальное число новых токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model: nn.Module, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение 4**. Модифицируйте функцию `generate`, чтобы она при семплировании учитывала\n",
    "   - $k$ наиболее вероятных токенов (параметр `top_k: int`)\n",
    "   - только токены, дающие в сумме вероятность не меньше $p$ (параметр `top_p: int`)\n",
    "   - температуру для `softmax`:\n",
    "\n",
    "\n",
    "      $\\begin{aligned}\\text{softmax}(x_i, \\tau) = \\frac{\\exp(x_i / \\tau)}{\\sum_j \\exp(x_i / \\tau)} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё понадобится функция, которая умеет декодировать выход из функции `generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_decode(out_tokens: Tensor) -> list[str]:\n",
    "    ...\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем несколько \"имён\" для проверки, начиная со $\\texttt{<BOS>}$ токена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generate(model, idx=torch.full(size=(4, 1), fill_value=1, dtype=int), max_new_tokens=40)\n",
    "print('\\n'.join(batch_decode(samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generate(model, idx=torch.full(size=(4, 1), fill_value=1, dtype=int), max_new_tokens=40)\n",
    "print('\\n'.join(batch_decode(samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Модифицируйте вычисление ошибки, чтобы не считать её для токенов, отвечающих за паддинг. Повлияло ли это на скорость обучения модели?\n",
    "6. Добавьте в генерацию входное условие: язык для генерируемого имени\n",
    "7. Используйте `nn.LSTM` и `nn.GRU` вместо самописных моделей, сравните результаты. \n",
    "8. Реализуйте модель для классификации имён по языкам"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
